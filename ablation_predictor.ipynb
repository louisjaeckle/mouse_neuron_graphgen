{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071fbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661d18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(sorted_list, target, ind):  # returns index of first value >=target\n",
    "    # sorted list is assumed to contain (x,y,z) coordinates. search is done based on ind coordinate (i.e. ind = 0 means x)\n",
    "    # returns len(sorted_list) if not found\n",
    "    lo = 0\n",
    "    hi = len(sorted_list) - 1\n",
    "    while lo < hi - 1:\n",
    "        mid = (lo + hi) // 2\n",
    "        if sorted_list[mid][ind] >= target:\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    if target > sorted_list[hi][ind]:\n",
    "        return len(sorted_list)\n",
    "    return lo if sorted_list[lo][ind] >= target else hi\n",
    "\n",
    "def smooth_data(data,kernel_size = 20):#smooth some time series data with kernel\n",
    "    kernel = np.ones(kernel_size)/kernel_size\n",
    "    return np.convolve(data,kernel,mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79086660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connectome:\n",
    "    # attr\n",
    "    # node_locations: maps node names to (x,y,z) coords\n",
    "    # names: list of node names\n",
    "    # coordinates: list of (x,y,z) coords, sorted by increasing x, with order corresponding to names\n",
    "    # ie node names[i] has coordinate coordinates[i]\n",
    "    def __init__(self, G):\n",
    "        self.G = G\n",
    "        self.n_nodes = G.number_of_nodes()\n",
    "        \n",
    "        self.types_dict = dict()# \"coarse\" label mapped to one-hot index\n",
    "        self.node_types = nx.get_node_attributes(G,'coarse')#node name mapped to coarse type\n",
    "        unique_types = set(self.node_types.values())\n",
    "        ind = 0\n",
    "        for soma_type in unique_types:\n",
    "            self.types_dict[soma_type] = ind\n",
    "            ind+=1\n",
    "        \n",
    "        \n",
    "        Xs = nx.get_node_attributes(G, 'x')\n",
    "        Ys = nx.get_node_attributes(G, 'y')\n",
    "        Zs = nx.get_node_attributes(G, 'z')\n",
    "        \n",
    "        self.node_dict = dict()\n",
    "        self.lo_bound = []  # [min x, min y, min z]\n",
    "        self.hi_bound = []  # [max x, max y, max z]\n",
    "        for node in list(G.nodes):\n",
    "            try:\n",
    "                if float(Xs[node]) > 80:\n",
    "                    continue\n",
    "                self.node_dict[node] = [float(Xs[node]), float(Ys[node]), float(Zs[node])]\n",
    "                if len(self.lo_bound) == 0:\n",
    "                    self.lo_bound = self.node_dict[node][:]\n",
    "                    self.hi_bound = self.node_dict[node][:]\n",
    "                else:\n",
    "                    for ind in range(3):\n",
    "                        self.lo_bound[ind] = min(self.lo_bound[ind], self.node_dict[node][ind])\n",
    "                        self.hi_bound[ind] = max(self.hi_bound[ind], self.node_dict[node][ind])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "        self.coordinates = []  # extract [x,y,z] nodes\n",
    "        self.names = []  # extract names\n",
    "        for node, coord in self.node_dict.items():\n",
    "            self.coordinates.append(coord)\n",
    "            self.names.append(node)\n",
    "        self.coordinates = np.asarray(self.coordinates)\n",
    "        self.names = np.asarray(self.names)\n",
    "        reorder = self.coordinates[:, 0].argsort()\n",
    "        self.coordinates = self.coordinates[reorder]\n",
    "        self.names = self.names[reorder]\n",
    "\n",
    "    #use this to 3d plot null and empirical networks\n",
    "    def plot_nodes(self,node_names,edges=None,display_labels=False):\n",
    "        #node_names - all nodes to be plotted\n",
    "        #edges- list of edges. if None, will be taken from empirical network\n",
    "\n",
    "        if len(node_names)==0:\n",
    "            return False\n",
    "\n",
    "\n",
    "        if edges==None:\n",
    "            edges = []\n",
    "            seen_nodes = set()\n",
    "            for node in node_names:\n",
    "                for neighbor in self.G.neighbors(node):\n",
    "                    if neighbor in seen_nodes:\n",
    "                        edges.append([node, neighbor])\n",
    "                seen_nodes.add(node)\n",
    "\n",
    "        x_plots = [self.node_dict[name][0] for name in node_names]\n",
    "        y_plots = [self.node_dict[name][1] for name in node_names]\n",
    "        z_plots = [self.node_dict[name][2] for name in node_names]\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "\n",
    "        ax.scatter(x_plots, y_plots, z_plots, s=30, color='blue')\n",
    "\n",
    "        if display_labels:\n",
    "            for node in node_names:\n",
    "                ax.text(*self.node_dict[node],node,size=10,zorder=1,color='k')\n",
    "\n",
    "        for node1, node2 in edges:\n",
    "            plt_args = [[self.node_dict[node1][i], self.node_dict[node2][i]] for i in range(3)]\n",
    "            ax.plot(*plt_args)\n",
    "        plt.show()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def plot_random_cube(self,cube_dim):\n",
    "        success = False\n",
    "        while not success:\n",
    "            bounds = self.get_random_bounds(cube_dim)\n",
    "            names, _ = self.get_cube(bounds,cube_dim)\n",
    "            success = self.plot_nodes(names)\n",
    "\n",
    "    # get random bounds for a cube to parse\n",
    "    def get_random_bounds(self, cube_dim):\n",
    "        center = [np.random.uniform(self.lo_bound[ind], self.hi_bound[ind]) for ind in range(3)]\n",
    "        return [coord - (cube_dim / 2) for coord in center]\n",
    "\n",
    "    def get_restricted_bounds(self,cube_dim,restriction,above):\n",
    "        #get bounds, not past z-coordinate give by restriction\n",
    "        #if above==True, bounds will be given such that cube lies above restriction. If above==False, cube will lie below restriction\n",
    "        center = [np.random.uniform(self.lo_bound[ind], self.hi_bound[ind]) for ind in range(2)]\n",
    "        if above:\n",
    "            center.append(np.random.uniform(restriction+(cube_dim/2),self.hi_bound[2]))\n",
    "        else:\n",
    "            center.append(np.random.uniform(self.lo_bound[2],restriction - (cube_dim / 2)))\n",
    "        return [coord - (cube_dim / 2) for coord in center]\n",
    "\n",
    "    # finds cube in network and returns nodes in cube\n",
    "    def get_cube(self, bounds, cube_dim):\n",
    "        # bounds = [xbound,ybound,zbound], cube dim is side size of square\n",
    "        # returns names, locations sorted by ascending z coordinates (ndarrays)\n",
    "        lo_ind, hi_ind = 0, 0\n",
    "        locations = self.coordinates\n",
    "        names = self.names\n",
    "\n",
    "        for ind in range(3):\n",
    "            lo_ind = binary_search(locations, bounds[ind], ind)\n",
    "            if lo_ind == -1:\n",
    "                #                 print('no nodes in range')#no nodes left\n",
    "                return np.array([]), np.array([])\n",
    "            hi_ind = binary_search(locations, bounds[ind] + cube_dim, ind)\n",
    "            if hi_ind <= lo_ind:\n",
    "                #                 print('no nodes in range')#no nodes left\n",
    "                return np.array([]), np.array([])\n",
    "            locations = locations[lo_ind:hi_ind]\n",
    "            names = names[lo_ind:hi_ind]\n",
    "            if (ind != 2):\n",
    "                reorder = locations[:, ind + 1].argsort()\n",
    "                locations = locations[reorder]\n",
    "                names = names[reorder]\n",
    "        return names, locations\n",
    "\n",
    "    # parse cube into model inputs and outputs\n",
    "    def parse_cube(self, names, locations, dim, keep_all_inputs,incl_prob=.5):\n",
    "        # takes nodes and parses into (dim x dim x dim) matrix, then extracts degree structure and adjacency matrix\n",
    "        # names is ndarray of node names, locations ndarray of coordinates\n",
    "        # formated like output of get_subgraph (ie sorted by ascending z coordinates)\n",
    "        # dim is size of cube, in terms of neurons per side\n",
    "        # return flattened arrays degrees,adjacencies,name which are representations of the degree structure and adjacency matrix and a list of names in order\n",
    "\n",
    "        if len(names) < dim ** 3:\n",
    "            #             print('not enough nodes to parse')\n",
    "            return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "        parsed_names = []\n",
    "        degrees = np.zeros((dim ** 3)+1)\n",
    "        degrees[-1] = 1#use degrees\n",
    "        adj_matrix = [[0 for _ in range(row_len)] for row_len in\n",
    "                      range(dim ** 3)]  # 2d list indexed by [higher node index],[lower node index]\n",
    "        node_indices = {}\n",
    "        for bott_ind in range(0, dim ** 3, dim ** 2):\n",
    "            layer_names = names[bott_ind:bott_ind + dim ** 2]\n",
    "            layer_locs = locations[bott_ind:bott_ind + dim ** 2]\n",
    "            reorder = layer_locs[:, 1].argsort()  # sort by y\n",
    "            layer_names = layer_names[reorder]\n",
    "            layer_locs = layer_locs[reorder]\n",
    "            for lo_ind in range(0, dim ** 2, dim):\n",
    "                row_names = layer_names[lo_ind:lo_ind + dim]\n",
    "                row_locs = layer_locs[lo_ind:lo_ind + dim]\n",
    "                reorder = row_locs[:,0].argsort()  # sort by x\n",
    "                row_names = row_names[reorder]\n",
    "                row_locs = row_locs[reorder]\n",
    "                for col in range(dim):\n",
    "                    node_index = bott_ind + lo_ind + col\n",
    "                    # get neighbors, update node_info and edges\n",
    "                    for neighbor in self.G.neighbors(row_names[col]):\n",
    "                        if neighbor in node_indices:\n",
    "                            degrees[node_index] += 1\n",
    "                            degrees[node_indices[neighbor]] += 1\n",
    "                            adj_matrix[node_index][node_indices[neighbor]] = 1\n",
    "                    node_indices[row_names[col]] = node_index\n",
    "                    parsed_names.append(row_names[col])\n",
    "        \n",
    "        #1=included, 0=not (last index)\n",
    "        clustering_input = np.asarray([np.random.uniform(),0])\n",
    "        soma_input = np.random.uniform(size=((dim**3*len(self.types_dict))+1))\n",
    "        soma_input[-1] = 0\n",
    "        if keep_all_inputs or np.random.uniform()<incl_prob:#use clustering?\n",
    "            clustering_input[1] = 1#use clustering!\n",
    "            edge_list = []\n",
    "            for n1 in range(dim**3):\n",
    "                for n2 in range(n1):\n",
    "                    if adj_matrix[n1][n2]==1:\n",
    "                        edge_list.append((parsed_names[n1],parsed_names[n2]))\n",
    "            subg = nx.Graph()\n",
    "            subg.add_nodes_from(parsed_names)\n",
    "            subg.add_edges_from(edge_list)\n",
    "            clustering_input[0] = nx.algorithms.cluster.transitivity(subg)\n",
    "\n",
    "        if keep_all_inputs or np.random.uniform()<incl_prob:#use soma types?\n",
    "            soma_input = np.zeros(((dim**3*len(self.types_dict))+1))\n",
    "            soma_input[-1] = 1#use soma types!\n",
    "            for node_ind in range(len(parsed_names)):\n",
    "                one_hot_ind = self.types_dict[self.node_types[parsed_names[node_ind]]]\n",
    "                soma_input[len(self.types_dict)*node_ind+one_hot_ind] = 1\n",
    "\n",
    "        if np.random.uniform()>incl_prob and not keep_all_inputs:#dont use degrees?\n",
    "            degrees = np.random.uniform(size=((dim**3)+1))\n",
    "            degrees[-1] = 0#dont use degrees\n",
    "        \n",
    "        features = np.concatenate((degrees,clustering_input,soma_input))\n",
    "        adj_matrix = [val for adj_row in adj_matrix for val in adj_row]\n",
    "        \n",
    "        return features, np.asarray(adj_matrix,dtype=np.float64),np.asarray(parsed_names)\n",
    "\n",
    "    # gets and parses subgraph in cube with given bounds\n",
    "    def get_and_parse(self, cube_dim, parse_dim, bounds,keep_all_inputs):\n",
    "        # return flattened ndarrays degrees,adjacencies, which are representations of the degree structure and adjacency matrix\n",
    "        names, locations = self.get_cube(bounds, cube_dim)\n",
    "        features, adjacencies,names = self.parse_cube(names, locations, parse_dim,keep_all_inputs)\n",
    "        return features, adjacencies,np.asarray(names)\n",
    "\n",
    "    # get flattened degrees, flattened adjacencies of a random cube\n",
    "    def random_get_and_parse(self, cube_dim, parse_dim,keep_all_inputs=False):\n",
    "        # random bound generation continues until viable one found\n",
    "        # cube_dim- side size of cube used in subnetwork\n",
    "        # parse dim- side dimension of 3d matrix to parse subgraph into\n",
    "        # return flattened ndarrays degrees,adjacencies, which are representations of the degree structure and adjacency matrix\n",
    "        while True:\n",
    "            bounds = self.get_random_bounds(cube_dim)\n",
    "            features, adjacencies,names = self.get_and_parse(cube_dim, parse_dim, bounds,keep_all_inputs)\n",
    "\n",
    "            if len(features) != 0:\n",
    "                return features, adjacencies,names\n",
    "\n",
    "    # get flattened degrees, flattened adjacencies of a random cube with restricted bounds\n",
    "    def restricted_get_and_parse(self,cube_dim,parse_dim,train_split,is_test,keep_all_inputs=False):\n",
    "        restriction = self.lo_bound[2]+((self.hi_bound[2]-self.lo_bound[2])*train_split)\n",
    "        while True:\n",
    "            bounds = self.get_restricted_bounds(cube_dim,restriction,is_test)#generate bounds above(is_test) or below(!is_test) restriction line\n",
    "            features, adjacencies, names = self.get_and_parse(cube_dim, parse_dim, bounds,keep_all_inputs)\n",
    "\n",
    "            if len(features) != 0:\n",
    "                return features, adjacencies, names\n",
    "\n",
    "    #generates batches for train and validation\n",
    "    def batch_generator(self,cube_dim,parse_dim,train_split,is_test,batch_size=32):\n",
    "        #randomly generate examples for training and testing\n",
    "        #train_split is fraction of 3d space that are used for testing, rest are used for validation\n",
    "        #train examples will be generated in the lower fraction of cube, corresponding to train_split\n",
    "        #validation examples will use upper fraction of cube (1-train_split)\n",
    "        n_nodes = int(parse_dim**3)#number of nodes per cube\n",
    "        input_size = (parse_dim**3)+1+2+((parse_dim**3)*len(self.types_dict))+1\n",
    "        \n",
    "        while True:\n",
    "            all_features = np.empty((batch_size,input_size))\n",
    "            all_labels = np.empty((batch_size,int(n_nodes*(n_nodes-1)/2)))\n",
    "            for i in range(batch_size):\n",
    "                features, labels,_ = self.restricted_get_and_parse(cube_dim, parse_dim,train_split,is_test)\n",
    "                all_features[i] = features\n",
    "                all_labels[i] = labels\n",
    "            yield all_features,all_labels\n",
    "        \n",
    "    def chung_lu_batch_generator(self,cube_dim,parse_dim,batch_size = 32):\n",
    "        input_size = (parse_dim**3)+1+2+((parse_dim**3)*len(self.types_dict))+1\n",
    "        \n",
    "        degrees = []\n",
    "        for _ in range(1000):\n",
    "            deg,a,n = self.random_get_and_parse(cube_dim,parse_dim)\n",
    "            degrees+=[d for d in deg[:parse_dim**3]]\n",
    "        degrees = np.asarray(degrees)\n",
    "        while True:\n",
    "            n_nodes = parse_dim**3\n",
    "            all_features = np.empty((batch_size,input_size))\n",
    "            all_labels = np.empty((batch_size,int((n_nodes*(n_nodes-1))/2)))\n",
    "            for i in range(batch_size):\n",
    "                sample_degrees = np.random.choice(degrees,size=(n_nodes,))\n",
    "                n_edges = np.sum(sample_degrees)/2\n",
    "                labels = []#adjacency scores for node pairs\n",
    "                for n1 in range(n_nodes):\n",
    "                    for n2 in range(n1):\n",
    "                        score = float(sample_degrees[n1]*sample_degrees[n2])/float((2*n_edges)-sample_degrees[n1])\n",
    "                        labels.append(min(1.0,score))\n",
    "                flag_and_clust = np.asarray([1,np.random.uniform(),0])#flag for degrees and random clustering val/flag\n",
    "                soma_input = np.random.uniform(size=((parse_dim**3)*len(self.types_dict)+1))\n",
    "                soma_input[-1] = 0\n",
    "                features = np.concatenate((sample_degrees,flag_and_clust,soma_input))\n",
    "                all_features[i] = np.asarray(features)\n",
    "                all_labels[i] = np.asarray(labels)\n",
    "            yield all_features,all_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6e22677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NullGenerator:\n",
    "    \n",
    "    def __init__(self,connectome,cube_dim,parse_dim):\n",
    "        \n",
    "        self.connectome = connectome\n",
    "        \n",
    "        self.parse_dim = parse_dim\n",
    "        self.cube_dim = cube_dim\n",
    "        \n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64,activation=tf.keras.layers.LeakyReLU()),\n",
    "            tf.keras.layers.Dense(32,activation=tf.keras.layers.LeakyReLU()),\n",
    "            tf.keras.layers.Dense((parse_dim**3)*(parse_dim**3-1)/2,use_bias=False)\n",
    "        ])\n",
    "    \n",
    "    def train(self,train_split=.5,batch_size=32,chung_epochs=0,train_epochs=500,steps_per_epoch=100,validation_steps=2,verbose=1,plot_metrics = True):\n",
    "        #train model. outputs train history\n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss = tf.keras.losses.MeanSquaredError(),\n",
    "            metrics = ['accuracy']\n",
    "        )\n",
    "        print(\"training on Chung-Lu\")\n",
    "        chung_generator = self.connectome.chung_lu_batch_generator(self.cube_dim,self.parse_dim,batch_size=batch_size)\n",
    "        self.model.fit(x=chung_generator,epochs=chung_epochs,steps_per_epoch=steps_per_epoch,verbose=verbose)\n",
    "        print(\"Done on Chung-Lu\")\n",
    "        print(\"training on empirical\")\n",
    "        train_generator = self.connectome.batch_generator(self.cube_dim,self.parse_dim,train_split,False)\n",
    "        validation_generator = self.connectome.batch_generator(self.cube_dim,parse_dim,train_split,True)\n",
    "        \n",
    "        hist = self.model.fit(x=train_generator, validation_data=validation_generator, epochs=train_epochs, validation_steps = validation_steps, steps_per_epoch=steps_per_epoch, verbose=verbose)\n",
    "        \n",
    "        if plot_metrics:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.plot(list(range(n_epochs)),hist.history['accuracy'],label='accuracy')\n",
    "            plt.plot(list(range(n_epochs)),hist.history['loss'],label='loss')\n",
    "            plt.plot(list(range(n_epochs)),hist.history['val_accuracy'],label='validation accuracy')\n",
    "            plt.plot(list(range(n_epochs)),hist.history['val_loss'],label='validation loss')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.xlabel('time step')\n",
    "            plt.ylabel('value')\n",
    "            plt.show()\n",
    "        return hist\n",
    "        \n",
    "    def cube_test(train_split, threshold=None):\n",
    "        features, adjacencies, names, locations = [],[],[],[]\n",
    "        while len(adjacencies)==0:\n",
    "            features,adjacencies,names = self.connectome.restricted_get_and_parse(self.cube_dim,self.parse_dim,train_split,True)\n",
    "        features = np.expand_dims(degrees,0)\n",
    "        predicted_adj = self.model(degrees)[0]\n",
    "        \n",
    "        #find the optimal threshold such that the number of predicted edges matches that of the empirical network\n",
    "        if threshold==None:\n",
    "            sorted_adj = np.copy(predicted_adj)\n",
    "            sorted_adj = np.sort(sorted_adj)\n",
    "            if sum(adjacencies)==0:\n",
    "                threshold = sorted_adj[-1]+1.0\n",
    "            else:\n",
    "                threshold = sorted_adj[len(sorted_adj)-int(sum(adjacencies))]\n",
    "        \n",
    "        loss = tf.keras.losses.MeanSquaredError()(adjacencies,predicted_adj)\n",
    "        print(f\"MSE loss: {loss}\")\n",
    "        predicted_edges = []\n",
    "        for i in range(1,len(names)):\n",
    "            adj_start = i*(i-1)/2\n",
    "            for prev_node in range(i):\n",
    "                adj_ind = int(prev_node+adj_start)\n",
    "                if predicted_adj[adj_ind]>=threshold:\n",
    "                    predicted_edges.append([names[i],names[prev_node]])\n",
    "        print(\"predicted graph: \")\n",
    "        self.connectome.plot_nodes(names,predicted_edges)\n",
    "        print(\"empirical graph: \")\n",
    "        self.connectome.plot_nodes(names)\n",
    "        \n",
    "        #plot ROC curve\n",
    "        aroc = 0.0\n",
    "        TPRs = [0.0]\n",
    "        FPRs = [0.0]\n",
    "        pred = predicted_adj.numpy()\n",
    "        reorder = pred.argsort()\n",
    "        pred = pred[reorder]\n",
    "        adjacencies = adjacencies[reorder]\n",
    "        n_pos = 0\n",
    "        total_pos = sum(adjacencies)\n",
    "        total_neg = len(adjacencies)-total_pos\n",
    "        for i in range(len(pred)-1,-1,-1):\n",
    "            if adjacencies[i]==1:#positive\n",
    "                n_pos+=1\n",
    "            TPRs.append(n_pos/total_pos)\n",
    "            FPRs.append((len(pred)-i-n_pos)/total_neg)\n",
    "            aroc+=TPRs[-1]*(FPRs[-1]-FPRs[-2])\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.plot(FPRs,TPRs)\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.title('ROC curve')\n",
    "        plt.show()\n",
    "        print(f'AROC: {aroc}')\n",
    "    \n",
    "    #get average loss and over ROC curve for niter predictions of the algo\n",
    "    def average_test(self,train_split,niter=5000):\n",
    "        all_losses, all_adj, all_pred_adj = [],[],[]\n",
    "        for iter in range(niter):\n",
    "            features, adjacencies, names, locations = [],[],[],[]\n",
    "            while len(adjacencies)==0:\n",
    "                features,adjacencies,names = self.connectome.restricted_get_and_parse(self.cube_dim,self.parse_dim,train_split,True,keep_all_inputs=True)\n",
    "            features = np.expand_dims(features,0)\n",
    "            predicted_adj = self.model(features)[0]\n",
    "            all_losses.append(tf.keras.losses.MeanSquaredError()(adjacencies,predicted_adj))\n",
    "            all_pred_adj.append(predicted_adj.numpy())\n",
    "            all_adj.append(adjacencies)\n",
    "        \n",
    "        #flatten and reorder arrays\n",
    "        all_adj = np.asarray(all_adj)\n",
    "        all_adj = all_adj.flatten()\n",
    "        all_pred_adj = np.asarray(all_pred_adj)\n",
    "        all_pred_adj = all_pred_adj.flatten()\n",
    "        reorder = all_pred_adj.argsort()\n",
    "        all_adj = all_adj[reorder]\n",
    "        all_pred_adj = all_pred_adj[reorder]\n",
    "        \n",
    "        self.plot_ROC(all_adj,all_pred_adj,f'Model ROC over {niter} iterations')\n",
    "        print(f'average loss: {np.mean(all_losses)}')\n",
    "    \n",
    "    \n",
    "    def average_chung_lu(self,train_split,niter=5000):\n",
    "        #this shit mad buggy for this parse function, feex it!\n",
    "        all_adj, all_pred_adj = [],[]\n",
    "        for iter in range(niter):\n",
    "            degrees, adjacencies, names, locations = [],[],[],[]\n",
    "            while len(adjacencies)==0:\n",
    "                features,adjacencies,names = self.connectome.restricted_get_and_parse(self.cube_dim,self.parse_dim,train_split,True)\n",
    "            degrees = features[:self.parse_dim**3]#extract degrees\n",
    "            pred_adj = []\n",
    "            n_edges = sum(adjacencies)\n",
    "            for i in range(len(degrees)):\n",
    "                for j in range(i):\n",
    "                    pred_adj.append((degrees[i]*degrees[j])/((2*n_edges)-degrees[i]))\n",
    "            all_pred_adj.append(pred_adj)\n",
    "            all_adj.append(adjacencies)\n",
    "        all_adj = np.asarray(all_adj)\n",
    "        all_adj = all_adj.flatten()\n",
    "        all_pred_adj = np.asarray(all_pred_adj)\n",
    "        all_pred_adj = all_pred_adj.flatten()\n",
    "        reorder = all_pred_adj.argsort()\n",
    "        all_adj = all_adj[reorder]\n",
    "        all_pred_adj = all_pred_adj[reorder]\n",
    "        self.plot_ROC(all_adj,all_pred_adj,f'Chung-Lu ROC over {niter} iterations')\n",
    "    \n",
    "    #plot ROC curve for pred_adj sequence\n",
    "    def plot_ROC(self,adj,pred_adj,title):\n",
    "        aroc = 0.0\n",
    "        TPRs = [0.0]\n",
    "        FPRs = [0.0]\n",
    "        n_pos = 0\n",
    "        total_pos = sum(adj)\n",
    "        total_neg = len(adj)-total_pos\n",
    "        for i in range(len(pred_adj)-1,-1,-1):\n",
    "            if adj[i]==1:#positive\n",
    "                n_pos+=1\n",
    "            TPRs.append(n_pos/total_pos)\n",
    "            FPRs.append((len(pred_adj)-i-n_pos)/total_neg)\n",
    "            aroc+=TPRs[-1]*(FPRs[-1]-FPRs[-2])\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.plot(FPRs,TPRs)\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        print(f'AROC: {aroc}')\n",
    "        \n",
    "    \n",
    "    def save_model(self,path):\n",
    "        self.model.save(path)\n",
    "    \n",
    "    def load_model(self,path):\n",
    "        self.model = tf.keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892d9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading. . .\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('reading. . .')\n",
    "G = nx.read_graphml('mouse_retina_1.graphml')\n",
    "G = G.to_undirected()#github says its undirected, and from/to is arbitrary, so ig do this\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c86d27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_dim = 23\n",
    "parse_dim = 3\n",
    "n_epochs = 500\n",
    "train_split = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70f0ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_connectome = Connectome(G)\n",
    "generator = NullGenerator(mouse_connectome,cube_dim,parse_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05da8884",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def subset_performance(subset,niter=500):#subset is boolean list [use degree, use clustering, use types]\n",
    "    all_adj, all_pred_adj,clustering_diffs = [],[],[]\n",
    "    for it in range(niter):\n",
    "        if it%50==0:\n",
    "            print(it)\n",
    "        features, adjacencies, names, locations = [],[],[],[]\n",
    "        while len(adjacencies)==0:\n",
    "            features,adjacencies,names = mouse_connectome.restricted_get_and_parse(cube_dim,parse_dim,train_split,True,keep_all_inputs=True)\n",
    "        \n",
    "        if not subset[0]:#dont use degree\n",
    "            for i in range(27):\n",
    "                features[i] = np.random.uniform()\n",
    "            features[27] = 0.0\n",
    "        if not subset[1]:#dont use clustering\n",
    "            features[28] = np.random.uniform()\n",
    "            features[29] = 0.0\n",
    "        if not subset[2]:#dont use types\n",
    "            for i in range(30,len(features)-1):\n",
    "                features[i] = np.random.uniform()\n",
    "                features[-1] = 0.0\n",
    "        features = np.expand_dims(features,0)\n",
    "        predicted_adj = generator.model(features)[0]\n",
    "        \n",
    "        sorted_adj = np.copy(predicted_adj)\n",
    "        sorted_adj = np.sort(sorted_adj)\n",
    "        threshold = 0\n",
    "        if sum(adjacencies)==0:\n",
    "            threshold = sorted_adj[-1]+1.0\n",
    "        else:\n",
    "            threshold = sorted_adj[len(sorted_adj)-int(sum(adjacencies))]\n",
    "        predicted_edges = []\n",
    "        actual_edges = []\n",
    "        for i in range(1,len(names)):\n",
    "            adj_start = i*(i-1)/2\n",
    "            for prev_node in range(i):\n",
    "                adj_ind = int(prev_node+adj_start)\n",
    "                if predicted_adj[adj_ind]>=threshold:\n",
    "                    predicted_edges.append([names[i],names[prev_node]])\n",
    "                if adjacencies[adj_ind]>.5:\n",
    "                    actual_edges.append([names[i],names[prev_node]])\n",
    "        true_subg = nx.Graph()\n",
    "        true_subg.add_nodes_from(names)\n",
    "        true_subg.add_edges_from(actual_edges)\n",
    "        \n",
    "        pred_subg = nx.Graph()\n",
    "        pred_subg.add_nodes_from(names)\n",
    "        pred_subg.add_edges_from(predicted_edges)\n",
    "        \n",
    "        clustering_diffs.append(abs(nx.algorithms.cluster.transitivity(true_subg)-nx.algorithms.cluster.transitivity(pred_subg)))\n",
    "        all_pred_adj.append(predicted_adj.numpy())\n",
    "        all_adj.append(adjacencies)\n",
    "\n",
    "    #flatten and reorder arrays\n",
    "    all_adj = np.asarray(all_adj)\n",
    "    all_adj = all_adj.flatten()\n",
    "    all_pred_adj = np.asarray(all_pred_adj)\n",
    "    all_pred_adj = all_pred_adj.flatten()\n",
    "    reorder = all_pred_adj.argsort()\n",
    "    all_adj = all_adj[reorder]\n",
    "    all_pred_adj = all_pred_adj[reorder]\n",
    "    generator.plot_ROC(all_adj,all_pred_adj,f'Model ROC over {niter} iterations')\n",
    "    print(f'average absolute difference in transitivity: {np.mean(clustering_diffs)}')\n",
    "    return all_adj,all_pred_adj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
